# STAR SentenceÂ ClassifierÂ ğŸ“„
```bash
python star_trainer.py \
  --csv_file data/cleaned_df.csv \
  --model_name google-bert/bert-base-uncased \
  --epochs 8 --lr 3e-5 --batch_size 32
```
A lightningâ€‘quick way to **train, evaluate, and analyse** a classifier that tags interview sentences with their STAR stageâ€”**Situation, Task, Action, Result**â€”using any HuggingÂ Face transformer.

---

## ğŸš€ QuickÂ Start

```bash
# 1. Create & activate a Python â‰¥3.10 env
uv sync && source .venv/bin/activate

# 2. Train ğŸ‹ï¸â€â™€ï¸
python star_trainer.py \
  --csv_file data/annotated.csv \
  --model_name google-bert/bert-base-uncased \
  --epochs 4 \
  --lr 3e-5 \
  --batch_size 32
```

Outputs land in `star_output/` (or your `--output_dir`):

```
loss_curve.png           # train vs eval loss per epoch
accuracy_curve.png       # eval accuracy per epoch
confusion_matrix.png     # 4Ã—4 heatâ€‘map
metrics.md               # copyâ€‘paste table for your Results section
best_model/              # saved HF model & tokenizer
```

---

## ğŸ› ï¸  CLI reference

| Flag           | Default             | Description                           |
| -------------- | ------------------- | ------------------------------------- |
| `--csv_file`   | **required**        | CSV with cols `sentence,label`        |
| `--model_name` | `bert-base-uncased` | HF model checkpoint                   |
| `--output_dir` | `star_output`       | Where checkpoints & plots are written |
| `--epochs`     | `5`                 | Training epochs                       |
| `--lr`         | `5e-5`              | AdamW learningâ€‘rate                   |
| `--batch_size` | `16`                | Perâ€‘device batch size                 |
| `--max_length` | `128`               | Token truncation length               |
| `--seed`       | `42`                | RNG seed for splits & PyTorch         |

### Recommended hyperâ€‘parameter presets

| Model                     | LR     | Batch | Notes                                   |
| ------------------------- | ------ | ----- | --------------------------------------- |
| `bert-base-uncased`       | `5e-5` | 16    | Good baseline. <2â€¯GB VRAM.              |
| `roberta-base`            | `3e-5` | 32    | Slightly better F1. Needs \~8â€¯GB VRAM.  |
| `deberta-v3-small`        | `2e-5` | 32    | Strong performance, slower training.    |
| `distilbert-base-uncased` | `8e-5` | 32    | Lightweight, use for rapid prototyping. |

> **TipÂ ğŸ’¡**: Reduce `--batch_size` if you hit CUDA OOM; compensate by increasing `--epochs`.

---

## ğŸ§ª Adding extras

* **Early stopping**: wrap `transformers.EarlyStoppingCallback` and add to `Trainer`.
* **CRF head**: replace the classification layer with `torchcrf.CRF` to model label order.
* **Crossâ€‘validation**: loop over folds in a shell or Python wrapper.
* **Data augmentation**: backâ€‘translate sentences or use synonym replacement.
* **Scheduler tweaks**: swap in `get_cosine_schedule_with_warmup` or a linear decay.
* **Inference**: load `best_model/` and call `model(**tokenizer(text, return_tensors="pt"))`.

---

## ğŸ“ Project layout

```
.
â”œâ”€â”€ data/                  # Your CSV lives here
â”œâ”€â”€ star_trainer.py        # Main training script (classâ€‘based)
â”œâ”€â”€ README.md              # You are hereÂ ğŸ‘‹
â””â”€â”€ star_output/           # Autoâ€‘generated artefacts
```

Made with â¤ï¸
